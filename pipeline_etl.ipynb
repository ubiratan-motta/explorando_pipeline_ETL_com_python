{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68716e5a-fd43-45bd-9714-e96ef031b927",
   "metadata": {},
   "source": [
    "O processo será todo pensado em usar orquestração de dados para poder deixar o pipeline rodando automáticamente e sozinho conforme a nescessidade de atualização (D-1, near real tima, e afins.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8279f460-6940-49d8-a537-4bed3300c2df",
   "metadata": {},
   "source": [
    "#EXTRACT\n",
    "- Estração dos dados de arquivos Jsons salvos em pastas diferentes dentro de um bucket da GCP\n",
    "- Os dados são carregados nesse bucket através de uma aplicação nativa da GCP chamada PUB/SUB\n",
    "- Será usado Python, Spark, PySpark e pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858ca387-53c7-457c-9f09-089e4f3b7c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################\n",
    "# #### Bibliotecas            ####\n",
    "# ################################\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark import SQLContext\n",
    "import argparse\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ################################\n",
    "# #### Parametros de Execucao ####\n",
    "# ################################\n",
    "\n",
    "script_name = os.path.basename(__file__)\n",
    "args = argparse.ArgumentParser(prog=script_name)\n",
    "args.add_argument('--p_dataref', required=True,\n",
    "                  help=\"Data de referencia para leitura dos dados de origem no formato YYYYMMDD (e.g., 20220530)\")\n",
    "args.add_argument('--p_ambiente', required=True, choices=['dev', 'hml', 'prd'],\n",
    "                  help=\"Sigla do ambiente de execucao (opcoes: dev, hml, prd)\")\n",
    "params = args.parse_args()\n",
    "print(\">>> Parametros de execucao informados: {}\".format(params))\n",
    "\n",
    "# prepara os parametros\n",
    "dataref = params.p_dataref\n",
    "dataref_ano = params.p_dataref[0:4]\n",
    "dataref_mes = params.p_dataref[4:6]\n",
    "dataref_dia = params.p_dataref[6:8]\n",
    "ambiente = params.p_ambiente\n",
    "fila = ['produto', 'produto_oferta', 'produto_status', 'emitir_nota']\n",
    "\n",
    "# ###########################################################################\n",
    "# #### Inicia Spark Session e Spark Context                              ####\n",
    "# ###########################################################################\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.logConf\", \"True\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = SparkContext.getOrCreate(SparkConf())\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "sqlContext.setConf(\"spark.sql.caseSensitive\", \"false\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set('spark.sql.session.timeZone', 'America/Sao_Paulo')\n",
    "\n",
    "# ###########################################################################\n",
    "# #### Inicializacao do Logger                                           ####\n",
    "# ###########################################################################\n",
    "Logger = sc._jvm.org.apache.log4j.Logger\n",
    "LOGGER = Logger.getLogger(__name__)\n",
    "\n",
    "LOGGER.info(\"Iniciando Processamento\")\n",
    "\n",
    "\n",
    "# ##################################################################\n",
    "# #### Mapa de campos de destino apos aplicar o flat das colunas####\n",
    "# #### (campo_destino)                                          ####\n",
    "# ##################################################################\n",
    "\n",
    "colunas = [ 'nomeProduto',\n",
    "            'dataInclusao',\n",
    "            'dataEstoque',\n",
    "            'dataHoraExecucao',\n",
    "            'quantidadeEstoque',\n",
    "            'quantidadeItens',\n",
    "            'codigoTipo',\n",
    "            'tipoProduto_codigo',\n",
    "            'tipoProduto_descricao',\n",
    "            'tipoProduto_descricaoReduzida',\n",
    "            'fornecedorProduto_nomeFornecedor',\n",
    "            'fornecedorProduto_empresa_codigo',\n",
    "            'fornecedorProduto_empresa_descricao',\n",
    "            'fornecedorProduto_empresa_descricaoReduzida',\n",
    "            'fornecedorProduto_responsavel_nome',\n",
    "            'fornecedorProduto_responsavel_telefone',\n",
    "            'fornecedorProduto_responsavel_email',\n",
    "            'fornecedorProduto_responsavel_cargo',\n",
    "            'fornecedorProduto_responsavel_endereco',\n",
    "            'fornecedorProduto_cnpj',\n",
    "            'fornecedorProduto_nomeFantasia',\n",
    "            'fornecedorProduto_dataInicio',\n",
    "            'flagPedidoMaximo',\n",
    "            'flagPedidoMinimo',\n",
    "            'quantidadeMinimaEstoque',\n",
    "            'quantidadeMaximaEstoque',\n",
    "            'valor_valorProduto',\n",
    "            'valor_valorProdutoDesconto',\n",
    "            'valor_comissaoVendedor',\n",
    "            'valor_maximoComissaoVendedor',\n",
    "            'valor_minimoComissaoVendedor'\n",
    "           ]\n",
    "\n",
    "\n",
    "# ##################################################################\n",
    "# #### Schema Json para leitura padronizada                     ####\n",
    "# ##################################################################\n",
    "\n",
    "structureSchema = StructType([\n",
    "    StructField('nomeProduto', StringType(), True),\n",
    "    StructField('dataInclusao', StringType(), True),\n",
    "    StructField('dataEstoque', StringType(), True),\n",
    "    StructField('dataHoraExecucao', StringType(), True),\n",
    "    StructField('quantidadeEstoque', StringType(), True),\n",
    "    StructField('quantidadeItens', StringType(), True),\n",
    "    StructField('codigoTipo', StringType(), True),\n",
    "    StructField('tipoProduto', StructType([\n",
    "        StructField('codigo', StringType(), True),\n",
    "        StructField('descricao', StringType(), True),\n",
    "        StructField('descricaoReduzida', StringType(), True)\n",
    "    ]), True),\n",
    "    StructField('fornecedorProduto', ArrayType(StructType([\n",
    "        StructField('nomeFornecedor', StringType(), True),\n",
    "        StructField('empresa', StructType([\n",
    "            StructField('codigo', StringType(), True),\n",
    "            StructField('descricao', StringType(), True),\n",
    "            StructField('descricaoReduzida', StringType(), True)\n",
    "        ]), True),\n",
    "        StructField('responsavel', StructType([\n",
    "            StructField('nome', StringType(), True),\n",
    "            StructField('telefone', StringType(), True),\n",
    "            StructField('email', StringType(), True),\n",
    "            StructField('cargo', StringType(), True),\n",
    "            StructField('endereco', StringType(), True)\n",
    "        ]), True),\n",
    "        StructField('cnpj', StringType(), True),\n",
    "        StructField('nomeFantasia', StringType(), True),\n",
    "        StructField('dataInicio', StringType(), True)\n",
    "    ]), True), True),\n",
    "    StructField('flagPedidoMaximo', StringType(), True),\n",
    "    StructField('flagPedidoMinimo', StringType(), True),\n",
    "    StructField('quantidadeMinimaEstoque', StringType(), True),\n",
    "    StructField('quantidadeMaximaEstoque', StringType(), True),\n",
    "    StructField('valor', StructType([\n",
    "        StructField('valorProduto', StringType(), True),\n",
    "        StructField('valorProdutoDesconto', StringType(), True),\n",
    "        StructField('comissaoVendedor', StringType(), True),\n",
    "        StructField('maximoComissaoVendedor', StringType(), True),\n",
    "        StructField('minimoComissaoVendedor', StringType(), True),\n",
    "    ]), True),\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b96ee7e-1299-40f7-9ee3-2f4ff82dd83d",
   "metadata": {},
   "source": [
    "#TRANSFORM\n",
    "- Aplicar flatten e explode para poder deixar de forma cartesiana todos os dados dos Jsons\n",
    "- Tranformar os dados: \n",
    "    - datatypes\n",
    "    - limpeza de sujeias\n",
    "    - verificação de validade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62c606e-a2a6-4e7b-a4dd-4fc26a2ffc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###########################################################################\n",
    "# #### Metodo: type_cols                                                 ####\n",
    "# #### Args: df_dtypes - dataframe a ser mapeado                         ####\n",
    "# ####       filter_type - tipo de campo se struct ou array              ####\n",
    "# ####                                                                   ####\n",
    "# #### Gera um array com todos os campos do dataframe com seus           ####\n",
    "# #### respectivos dataypes                                              ####\n",
    "# ###########################################################################\n",
    "\n",
    "def type_cols(df_dtypes, filter_type):\n",
    "    cols = []\n",
    "    for col_name, col_type in df_dtypes:\n",
    "        if col_type.startswith(filter_type):\n",
    "            cols.append(col_name)\n",
    "    return cols\n",
    "\n",
    "\n",
    "# ###########################################################################\n",
    "# #### Metodo: flatten_df                                                ####\n",
    "# #### Args: nested_df - dataframe que devera sofrer o flatten           ####\n",
    "# ####       sep - separador de campos                                   ####\n",
    "# ####                                                                   ####\n",
    "# #### Retorna um dataframe colunado quando tipo de estrutura for struct ####\n",
    "# ###########################################################################\n",
    "\n",
    "def flatten_df(nested_df, sep='_'):\n",
    "    nested_cols = [c[0] for c in nested_df.dtypes if c[1][:6] == 'struct']\n",
    "    flatten_cols = [fc for fc, _ in nested_df.dtypes if fc not in nested_cols]\n",
    "    for nc in nested_cols:\n",
    "        for cc in nested_df.select(f'{nc}.*').columns:\n",
    "            if sep is None:\n",
    "                flatten_cols.append(F.col(f'{nc}.{cc}').alias(f'{cc}'))\n",
    "            else:\n",
    "                flatten_cols.append(F.col(f'{nc}.{cc}').alias(f'{nc}{sep}{cc}'))\n",
    "    return nested_df.select(flatten_cols)\n",
    "\n",
    "\n",
    "# ###########################################################################\n",
    "# #### Metodo: explode_df                                                ####\n",
    "# #### Args: nested_df - dataframe que devera sofrer o explode           ####\n",
    "# ####                                                                   ####\n",
    "# #### Retorna um dataframe colunado quando tipo de estrutura for array  ####\n",
    "# ###########################################################################\n",
    "\n",
    "def explode_df(nested_df):\n",
    "    array_cols = [c[0] for c in nested_df.dtypes if c[1][:5] == 'array']\n",
    "    exploded_df = nested_df\n",
    "    for nc in array_cols:\n",
    "        exploded_df = exploded_df.withColumn(nc, F.explode_outer(F.col(nc)))\n",
    "    return exploded_df\n",
    "\n",
    "\n",
    "# ###########################################################################\n",
    "# #### Metodo: flatten_explode_df                                        ####\n",
    "# #### Args: nested_df - dataframe que origem                            ####\n",
    "# ####                                                                   ####\n",
    "# #### Separa o dataframe origem por tipo de estrutura array ou struct   ####\n",
    "# #### para aplicacao do metodo mais adequado de padronizacao            ####\n",
    "# ###########################################################################\n",
    "\n",
    "def flatten_explode_df(nested_df):\n",
    "    df = nested_df\n",
    "    struct_cols = type_cols(nested_df.dtypes, 'struct')\n",
    "    array_cols = type_cols(nested_df.dtypes, 'array')\n",
    "    if array_cols:\n",
    "        df = explode_df(df)\n",
    "        return flatten_explode_df(df)\n",
    "    if struct_cols:\n",
    "        df = flatten_df(df)\n",
    "        return flatten_explode_df(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ##################################################################\n",
    "# #### Cria um dataframe vazio com o schema Json definido       ####\n",
    "# ##################################################################\n",
    "LOGGER.info(\"Criando um dataframe vazio com o schema Json definido\")\n",
    "\n",
    "schemaflat = StructType([\n",
    "    StructField('nomeProduto', StringType(), True),\n",
    "    StructField('dataInclusao', StringType(), True),\n",
    "    StructField('dataEstoque', StringType(), True),\n",
    "    StructField('dataHoraExecucao', StringType(), True),\n",
    "    StructField('quantidadeEstoque', StringType(), True),\n",
    "    StructField('quantidadeItens', StringType(), True),\n",
    "    StructField('codigoTipo', StringType(), True),\n",
    "    StructField('tipoProduto_codigo', StringType(), True),\n",
    "    StructField('tipoProduto_descricao', StringType(), True),\n",
    "    StructField('tipoProduto_descricaoReduzida', StringType(), True),\n",
    "    StructField('fornecedorProduto_nomeFornecedor', StringType(), True),\n",
    "    StructField('fornecedorProduto_empresa_codigo', StringType(), True),\n",
    "    StructField('fornecedorProduto_empresa_descricao', StringType(), True),\n",
    "    StructField('fornecedorProduto_empresa_descricaoReduzida', StringType(), True),\n",
    "    StructField('fornecedorProduto_responsavel_nome', StringType(), True),\n",
    "    StructField('fornecedorProduto_responsavel_telefone', StringType(), True),\n",
    "    StructField('fornecedorProduto_responsavel_email', StringType(), True),\n",
    "    StructField('fornecedorProduto_responsavel_cargo', StringType(), True),\n",
    "    StructField('fornecedorProduto_responsavel_endereco', StringType(), True),\n",
    "    StructField('fornecedorProduto_cnpj', StringType(), True),\n",
    "    StructField('fornecedorProduto_nomeFantasia', StringType(), True),\n",
    "    StructField('fornecedorProduto_dataInicio', StringType(), True),\n",
    "    StructField('flagPedidoMaximo', StringType(), True),\n",
    "    StructField('flagPedidoMinimo', StringType(), True),\n",
    "    StructField('quantidadeMinimaEstoque', StringType(), True),\n",
    "    StructField('quantidadeMaximaEstoque', StringType(), True),\n",
    "    StructField('valor_valorProduto', StringType(), True),\n",
    "    StructField('valor_valorProdutoDesconto', StringType(), True),\n",
    "    StructField('valor_comissaoVendedor', StringType(), True),\n",
    "    StructField('valor_maximoComissaoVendedor', StringType(), True),\n",
    "    StructField('valor_minimoComissaoVendedor', StringType(), True),\n",
    "    StructField('NOM_EVENTO', StringType(), True)\n",
    "])\n",
    "\n",
    "df_cota = spark.createDataFrame([], schemaflat)\n",
    "\n",
    "# ##################################################################\n",
    "# #### Criando um dataframe formato texto com jsons do dia      ####\n",
    "# #### anterior para cada fila                                  ####\n",
    "# ##################################################################\n",
    "\n",
    "LOGGER.info(\"Aplicando tratamentos de flatten e explode para os topicos de cotacao\")\n",
    "\n",
    "for line in fila:\n",
    "    topico = line\n",
    "    path_orig = \"gs://business-analytics-{}_staging/auto20/cotacao/cotacao/api/{}/json/{}/{}/{}/*.json\".format(\n",
    "        ambiente, topico, dataref_ano, dataref_mes, dataref_dia)\n",
    "    df = sqlContext.read.json(path_orig, schema=structureSchema)\n",
    "    df_flat = flatten_explode_df(df)\n",
    "    for column in [column for column in colunas if column not in df_flat.columns]:\n",
    "        df_flat = df_flat.withColumn(column, F.lit(''))\n",
    "    dfnew_flat = df_flat.select(colunas)\n",
    "    dfnew_flat = dfnew_flat.withColumn('NOM_EVENTO', F.lit(topico))\n",
    "    df_unic = dfnew_flat.dropDuplicates()\n",
    "    df_cota = df_cota.union(df_unic)\n",
    "\n",
    "# ##################################################################\n",
    "# #### Filtra json com dataHoraExecucao null - layout errado    ####\n",
    "# #### Filtra json com nomeProduto null - layout errado         ####\n",
    "# ##################################################################\n",
    "\n",
    "df_cota = df_cota.filter(df_cota.dataHoraExecucao.isNotNull())\n",
    "df_cota = df_cota.filter(df_cota.nomeProduto.isNotNull())\n",
    "\n",
    "# ##################################################################\n",
    "# #### Tratamento de mapeamento do produto                      ####\n",
    "# ##################################################################\n",
    "\n",
    "LOGGER.info(\"Renomeando DataFrame colunado e organizando os DataTypes\")\n",
    "\n",
    "df_fim = df_cota.select(F.coalesce(F.upper(df_cota.nomeProduto.cast('string').alias(\"NM_PRODUTO\"))),\n",
    "                        (df_cota.dataInclusao).cast(\"timestamp\").alias(\"DAT_INCLUSAO_SISTEMA\"),\n",
    "                        (df_cota.dataEstoque).cast(\"timestamp\").alias(\"DAT_ENTRADA_ESTOQUE\"),\n",
    "                        (df_cota.dataHoraExecucao / 1000).cast(\"timestamp\").alias(\"DAT_EXECUCAO\"),\n",
    "                        df_cota.quantidadeEstoque.cast('long').alias(\"QNT_ESTOQUE\"),\n",
    "                        df_cota.quantidadeItens.cast('long').alias(\"QNT_ITENS_ESTOQUE\"),\n",
    "                        df_cota.codigoTipo.cast('long').alias(\"COD_TIPO\"),\n",
    "                        df_cota.tipoProduto_codigo.cast('long').alias(\"COD_TIPO_PRODUTO\"),\n",
    "                        F.coalesce(F.upper(df_cota.tipoProduto_codigo.cast('string').alias(\"DES_TIPO_PRODUTO\"))),\n",
    "                        F.coalesce(F.upper(df_cota.tipoProduto_codigo.cast('string').alias(\"DES_ABREVIADO_TIPO_PRODUTO\"))),\n",
    "                        F.coalesce(F.upper(df_cota.fornecedorProduto_nomeFornecedor.cast('string').alias(\"NM_FORNECEDOR\"))),\n",
    "                        df_cota.fornecedorProduto_empresa_codigo.cast('long').alias(\"COD_EMPRESA\"),\n",
    "                        F.coalesce(F.upper(df_cota.fornecedorProduto_empresa_descricao.cast('string').alias(\"DES_EMPRESA\"))),\n",
    "                        F.coalesce(F.upper(df_cota.fornecedorProduto_empresa_descricaoReduzida.cast('string').alias(\"DES_ABREVIADOR_EMPRESA\"))),\n",
    "                        F.coalesce(F.upper(df_cota.fornecedorProduto_responsavel_nome.cast('string').alias(\"NM_RESPONSAVEL_FORNECEDOR\"))),\n",
    "                        (df_cota.fornecedorProduto_responsavel_telefone.cast('long').alias(\"NUM_TELEFONE_RESPONSAVEL_FORNECEDOR\"),\n",
    "                        F.coalesce(F.upper(df_cota.fornecedorProduto_responsavel_email.cast('string').alias(\"NM_EMAIL_RESPONSAVEL_FORNECEDOR\"))),\n",
    "                        F.coalesce(F.upper(df_cota.fornecedorProduto_responsavel_cargo.cast('string').alias(\"NM_CARGO_RESPONSAVEL_FORNECEDOR\"))),\n",
    "                        F.coalesce(F.upper(df_cota.fornecedorProduto_responsavel_endereco.cast('string').alias(\"NM_ENDERECO_RESPONSAVEL_FORNECEDOR\"))),\n",
    "                        (df_cota.fornecedorProduto_cnpj.cast('long').alias(\"NUM_CNPJ_FORNECEDOR\"),\n",
    "                        F.coalesce(F.upper(df_cota.fornecedorProduto_nomeFantasia.cast('string').alias(\"NM_FANTASIA_FORNECEDOR\"))),\n",
    "                        (df_cota.fornecedorProduto_dataInicio).cast(\"timestamp\").alias(\"DAT_INICIO_FORNECEDOR\"),\n",
    "                        F.coalesce(F.upper(df_cota.flagPedidoMaximo.cast('string').alias(\"FLG_PEDIDO_MAXIMO\"))),\n",
    "                        F.coalesce(F.upper(df_cota.flagPedidoMinimo.cast('string').alias(\"FLG_PEDIDO_MINIMO\"))),\n",
    "                        (df_cota.quantidadeMinimaEstoque.cast('long').alias(\"QNT_MINIMA_ESTOQUE\"),\n",
    "                        (df_cota.quantidadeMaximaEstoque.cast('long').alias(\"QNT_MAXIMA_ESTOQUE\"),\n",
    "                        (df_cota.valor_valorProduto.cast('decimal(19,5)').alias(\"VLR_PRODUTO\"),\n",
    "                        (df_cota.valor_valorProdutoDesconto.cast('decimal(19,5)').alias(\"VLR_PRODUTO_DESCONTO\"),\n",
    "                        (df_cota.valor_comissaoVendedor.cast('decimal(19,5)').alias(\"VLR_COMISSAO_VENDEDOR\"),\n",
    "                        (df_cota.valor_maximoComissaoVendedor.cast('decimal(19,5)').alias(\"VLR_MAXIMO_COMISSAO_VENDEDOR\"),\n",
    "                        (df_cota.valor_minimoComissaoVendedor.cast('decimal(19,5)').alias(\"VLR_MINIMO_COMISSAO_VENDEDOR\"),\n",
    "                        F.upper(df_cota.NOM_EVENTO).alias(\"NOM_EVENTO\"),\n",
    "                        F.date_format(F.from_unixtime(df_cota.dataHoraExecucao / 1000), \"yyyyMMdd\").cast(\"string\").alias(\"COD_PARTICIONAMENTO\")\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e39f956-8cc2-4607-a0c1-17c580480302",
   "metadata": {},
   "source": [
    "#LOAD\n",
    "- Carregar os dados após transformados em uma tabela do BigQuery, assim permitindo consulta das areas interessadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be2f20c-4127-4834-bf24-7078c1d181d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################\n",
    "# #### Converte dataframe unificado em Pandas                   ####\n",
    "# ##################################################################\n",
    "\n",
    "dfP_fim = df_fim.select(\"*\").toPandas()\n",
    "\n",
    "# ##################################################################\n",
    "# #### Insere registros de emissao na tabela do BigQuery        ####\n",
    "# #### Dataset: ANALITICO_CONTROLE_LOJA                         ####\n",
    "# #### tabela: CONSOLIDADO_PRODUTO                              ####\n",
    "# ##################################################################\n",
    "LOGGER.info(\"Iniciando Insert\")\n",
    "bigqueryClient = bigquery.Client()\n",
    "tableRef = bigqueryClient.dataset(\"ANALITICO_CONTROLE_LOJA\").table(\"CONSOLIDADO_PRODUTO\")\n",
    "bigqueryJob = bigqueryClient.load_table_from_dataframe(dfP_fim, tableRef)\n",
    "bigqueryJob.result()\n",
    "\n",
    "LOGGER.info(\"Registros inseridos com sucesso\")\n",
    "\n",
    "LOGGER.info(\"Fim de processamento\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
